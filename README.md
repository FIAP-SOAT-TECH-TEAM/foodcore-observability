# ğŸ“Š FoodCore Observability

<div align="center">
 
Stack de observabilidade para monitoramento de microsserviÃ§os do projeto FoodCore. Desenvolvida como parte do curso de Arquitetura de Software da FIAP (Tech Challenge).

</div>

<div align="center">
  <a href="#visao-geral">VisÃ£o Geral</a> â€¢
  <a href="#stack">Stack de Observabilidade</a> â€¢
  <a href="#servicos-expostos">ServiÃ§os Expostos</a> â€¢
  <a href="#infra">Infraestrutura</a> â€¢
  <a href="#limitacoes-quota">LimitaÃ§Ãµes de quotas</a> â€¢
  <a href="#deploy">Fluxo de Deploy</a> â€¢
  <a href="#instalacao-e-uso">InstalaÃ§Ã£o e Uso</a> â€¢
  <a href="#debitos-tecnicos">DÃ©bitos TÃ©cnicos</a> â€¢
  <a href="#contribuicao">ContribuiÃ§Ã£o</a>
</div><br>

> ğŸ“½ï¸ VÃ­deo de demonstraÃ§Ã£o da arquitetura: [https://youtu.be/k3XbPRxmjCw](https://youtu.be/k3XbPRxmjCw)<br>

---

<h2 id="visao-geral">ğŸ“‹ VisÃ£o Geral</h2>

Este repositÃ³rio contÃ©m os scripts **Terraform** e o **Helm Chart** responsÃ¡veis por provisionar toda a stack de observabilidade do projeto FoodCore no cluster AKS.

### TrÃªs Pilares da Observabilidade

| Pilar | Stack | DescriÃ§Ã£o |
|-------|-------|-----------|
| **Logs** | EFK | Elasticsearch, Fluentd, Kibana |
| **MÃ©tricas** | Prometheus + Grafana | Coleta e visualizaÃ§Ã£o de mÃ©tricas |
| **Traces** | Zipkin | Rastreamento distribuÃ­do |

---

<h2 id="stack">ğŸ”­ Stack de Observabilidade</h2>

### ğŸ“‹ Logs - EFK Stack

| Componente | DescriÃ§Ã£o | VersÃ£o |
|------------|-----------|--------|
| **Elasticsearch** | Armazenamento e indexaÃ§Ã£o de logs | 8.13.4 |
| **Fluentd** | Coleta e agregaÃ§Ã£o de logs dos containers | v1.18 |
| **Kibana** | VisualizaÃ§Ã£o e anÃ¡lise de logs | 8.13.4 |

**Funcionamento atual**:

- Logs enviados para stdout/stderr pelos microsserviÃ§os (SLF4J)
- Containerd redireciona para diretÃ³rio de logs
- Fluentd (DaemonSet) consome e envia para Elasticsearch

### ğŸ“ˆ MÃ©tricas - Prometheus + Grafana

| Componente | DescriÃ§Ã£o |
|------------|-----------|
| **Prometheus** | Coleta e armazenamento de mÃ©tricas via scraping |
| **Grafana** | Dashboards e visualizaÃ§Ã£o |

> ğŸ“Š Inclui dashboard **JVM Micrometer** prÃ©-configurado para aplicaÃ§Ãµes Spring Boot.

### ğŸ‘£ Traces - Zipkin

| Componente | DescriÃ§Ã£o |
|------------|-----------|
| **Zipkin** | Rastreamento distribuÃ­do de requisiÃ§Ãµes |

**Funcionamento atual**:

- Auto-instrumentaÃ§Ã£o via Micrometer Tracing
- Spring Actuator expÃµe mÃ©tricas para Prometheus

---

<h2 id="servicos-expostos">ğŸ“¡ ServiÃ§os Expostos</h2>

| ServiÃ§o | Path | Ingress Port |
|---------|------|--------------|
| Kibana | `/kibana` | 80 (Http) |
| Prometheus | `/prometheus` | 80 (Http) |
| Grafana | `/grafana` | 80 (Http) |
| Zipkin | `/zipkin` | 80 (Http) |

> âš ï¸ A URL Base pode ser obtida via output terraform `aks_ingress_public_ip_fqdn` (foodcore-infra).

---

<h2 id="infra">ğŸŒ Infraestrutura</h2>

<details>
<summary>Expandir para mais detalhes</summary>

### Recursos Kubernetes

| Recurso | DescriÃ§Ã£o |
|---------|-----------|
| **Elasticsearch** | StatefulSet | Volume persistente (3Gi) |
| **Fluentd** | DaemonSet | Coleta em todos os nodes |
| **Kibana** | Deployment | Com Ingress configurado |
| **Prometheus** | Deployment | ConfigMap de scrape configs |
| **Grafana** | Deployment | Datasources e dashboards prÃ©-configurados |
| **Zipkin** | Deployment | Distributed tracing |
| **StorageClass** | - | Azure Disk para volumes |
| **Ingress** | - | Roteamento via Azure Application Gateway (LB Layer 7) |

- O **Application Gateway** recebe trÃ¡fego em um **Frontend IP PÃºblico**
- Roteamento direto para os IPs dos Pods (**Azure CNI + Overlay**)
- Path exposto: `/`

> âš ï¸ ApÃ³s o deploy (CD), aguarde cerca de **5 minutos** para que o **AGIC** finalize a configuraÃ§Ã£o do Application Gateway.

### IntegraÃ§Ãµes

| ServiÃ§o | Tipo | DescriÃ§Ã£o |
|---------|------|-----------|
| **Azure Service Bus** | AssÃ­ncrona | PublicaÃ§Ã£o/consumo de eventos |
| **PostgreSQL** | SÃ­ncrona | PersistÃªncia de dados |
| **FoodCore Catalog** | HTTP | ValidaÃ§Ã£o de produtos |

### ğŸ” Azure Key Vault Provider (CSI)

- Sincroniza secrets do Azure Key Vault com Secrets do Kubernetes
- Monta volumes CSI com `tmpfs` dentro dos Pods
- Utiliza o CRD **SecretProviderClass**

> âš ï¸ Caso o valor de uma secret seja alterado no Key Vault, Ã© necessÃ¡rio **reiniciar os Pods**, pois variÃ¡veis de ambiente sÃ£o injetadas apenas na inicializaÃ§Ã£o.
>
> ReferÃªncia: <https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-configuration-options>

### Observabilidade

- **Logs**: Envio para Elasticsearch via Fluentd
- **MÃ©tricas**: ExposiÃ§Ã£o para Prometheus via Micrometer
- **Tracing**: InstrumentaÃ§Ã£o com Zipkin
- **Dashboards**: VisualizaÃ§Ã£o no Grafana

</details>

---

<h2 id="limitacoes-quota">ğŸ“‰ LimitaÃ§Ãµes de Quota (Azure for Students)</h2>

<details>
<summary>Expandir para mais detalhes</summary>

> A assinatura **Azure for Students** impÃµe as seguintes restriÃ§Ãµes:
>
> - **RegiÃ£o**: Brazil South nÃ£o estÃ¡ disponÃ­vel. Utilizamos **South Central US** como alternativa
>
> - **Quota de VMs**: Apenas **2 instÃ¢ncias** do SKU utilizado para o node pool do AKS, tendo um impacto direto na escalabilidade do cluster. Quando o limite Ã© atingido, novos nÃ³s nÃ£o podem ser criados e dÃ£o erro no provisionamento de workloads.
>
> ### Erro no CD dos MicrosserviÃ§os
>
> Durante o deploy dos microsserviÃ§os, Pods podem ficar com status **Pending** e o seguinte erro pode aparecer:
>
> <img src=".github/images/error.jpeg" alt="Error" />
> <img src=".github/images/erroDeploy.jpeg" alt="Error" />
>
> **Causa**: O cluster atingiu o limite mÃ¡ximo de VMs permitido pela quota e nÃ£o hÃ¡ recursos computacionais (CPU/memÃ³ria) disponÃ­veis nos nÃ³s existentes.
>
> **SoluÃ§Ã£o**: Aguardar a liberaÃ§Ã£o de recursos de outros pods e reexecutar CI + CD.

</details>

---

<h2 id="deploy">âš™ï¸ Fluxo de Deploy</h2>

<details>
<summary>Expandir para mais detalhes</summary>

### Pipeline

1. **Pull Request**
   - Preencher template de pull request adequadamente

2. **RevisÃ£o e AprovaÃ§Ã£o**
   - MÃ­nimo 1 aprovaÃ§Ã£o de CODEOWNER

3. **Merge para Main**

### ProteÃ§Ãµes

- Branch `main` protegida
- Nenhum push direto permitido
- Todos os checks devem passar

### Ordem de Provisionamento

```
1. foodcore-infra        (AKS, VNET)
2. foodcore-db           (Bancos de dados)
3. foodcore-auth           (Azure Function Authorizer)
4. foodcore-observability (ServiÃ§os de Observabilidade)
5. foodcore-order            (MicrosserviÃ§o de pedido)
6. foodcore-payment            (MicrosserviÃ§o de pagamento)
7. foodcore-catalog            (MicrosserviÃ§o de catÃ¡logo)
```

> âš ï¸ Opcionalmente, as pipelines do repositÃ³rio `foodcore-shared` podem ser executadas para publicaÃ§Ã£o de um novo package. Atualizar os microsserviÃ§os para utilizarem a nova versÃ£o do pacote.

</details>

---

<h2 id="instalacao-e-uso">ğŸš€ InstalaÃ§Ã£o e Uso</h2>

### Desenvolvimento Local

```bash
# Clonar repositÃ³rio
git clone https://github.com/FIAP-SOAT-TECH-TEAM/foodcore-observability.git
cd foodcore-observability

# Configurar variÃ¡veis de ambiente (Docker)
cp docker/env-example docker/.env

# Subir dependÃªncias
./food start:infra
```

---

<h2 id="debitos-tecnicos">âš ï¸ DÃ©bitos TÃ©cnicos</h2>

<details>
<summary>Expandir para mais detalhes</summary>

| DÃ©bito | DescriÃ§Ã£o | Impacto |
|--------|-----------|---------|
| **OpenTelemetry** | Migrar de Micrometer para OpenTelemetry | PadronizaÃ§Ã£o de observabilidade |
| **APM** | Usar uma ferramenta de APM ao invÃ©s de serviÃ§os isolados | Ferramenta unificada de observabilidade |

</details>

---

<h2 id="contribuicao">ğŸ¤ ContribuiÃ§Ã£o</h2>

### Fluxo de ContribuiÃ§Ã£o

1. Crie uma branch a partir de `main`
2. Implemente suas alteraÃ§Ãµes
3. Abra um Pull Request
4. Aguarde aprovaÃ§Ã£o de um CODEOWNER

### LicenÃ§a

Este projeto estÃ¡ licenciado sob a [MIT License](LICENSE).

---

<div align="center">
  <strong>FIAP - PÃ³s-graduaÃ§Ã£o em Arquitetura de Software</strong><br>
  Tech Challenge 4
</div>
